{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOYBsrzV1Mi0"
   },
   "source": [
    "#Submitted by:\n",
    "\n",
    "Chigozie Kenneth Okafor 225983\n",
    "\n",
    "Md Khamar Uz Zama 226267\n",
    "\n",
    "Rajatha Nagaraja Rao 223758\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c9V7dCEvueYZ",
    "outputId": "f1fc8c0b-22b5-441e-b0fa-4a97de9a1694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-26 09:09:03.873652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "Split input into 22981 sequences...\n",
      "Serialized 100 sequences...\n",
      "Serialized 200 sequences...\n",
      "Serialized 300 sequences...\n",
      "Serialized 400 sequences...\n",
      "Serialized 500 sequences...\n",
      "Serialized 600 sequences...\n",
      "Serialized 700 sequences...\n",
      "Serialized 800 sequences...\n",
      "Serialized 900 sequences...\n",
      "Serialized 1000 sequences...\n",
      "Serialized 1100 sequences...\n",
      "Serialized 1200 sequences...\n",
      "Serialized 1300 sequences...\n",
      "Serialized 1400 sequences...\n",
      "Serialized 1500 sequences...\n",
      "Serialized 1600 sequences...\n",
      "Serialized 1700 sequences...\n",
      "Serialized 1800 sequences...\n",
      "Serialized 1900 sequences...\n",
      "Serialized 2000 sequences...\n",
      "Serialized 2100 sequences...\n",
      "Serialized 2200 sequences...\n",
      "Serialized 2300 sequences...\n",
      "Serialized 2400 sequences...\n",
      "Serialized 2500 sequences...\n",
      "Serialized 2600 sequences...\n",
      "Serialized 2700 sequences...\n",
      "Serialized 2800 sequences...\n",
      "Serialized 2900 sequences...\n",
      "Serialized 3000 sequences...\n",
      "Serialized 3100 sequences...\n",
      "Serialized 3200 sequences...\n",
      "Serialized 3300 sequences...\n",
      "Serialized 3400 sequences...\n",
      "Serialized 3500 sequences...\n",
      "Serialized 3600 sequences...\n",
      "Serialized 3700 sequences...\n",
      "Serialized 3800 sequences...\n",
      "Serialized 3900 sequences...\n",
      "Serialized 4000 sequences...\n",
      "Serialized 4100 sequences...\n",
      "Serialized 4200 sequences...\n",
      "Serialized 4300 sequences...\n",
      "Serialized 4400 sequences...\n",
      "Serialized 4500 sequences...\n",
      "Serialized 4600 sequences...\n",
      "Serialized 4700 sequences...\n",
      "Serialized 4800 sequences...\n",
      "Serialized 4900 sequences...\n",
      "Serialized 5000 sequences...\n",
      "Serialized 5100 sequences...\n",
      "Serialized 5200 sequences...\n",
      "Serialized 5300 sequences...\n",
      "Serialized 5400 sequences...\n",
      "Serialized 5500 sequences...\n",
      "Serialized 5600 sequences...\n",
      "Serialized 5700 sequences...\n",
      "Serialized 5800 sequences...\n",
      "Serialized 5900 sequences...\n",
      "Serialized 6000 sequences...\n",
      "Serialized 6100 sequences...\n",
      "Serialized 6200 sequences...\n",
      "Serialized 6300 sequences...\n",
      "Serialized 6400 sequences...\n",
      "Serialized 6500 sequences...\n",
      "Serialized 6600 sequences...\n",
      "Serialized 6700 sequences...\n",
      "Serialized 6800 sequences...\n",
      "Serialized 6900 sequences...\n",
      "Serialized 7000 sequences...\n",
      "Serialized 7100 sequences...\n",
      "Serialized 7200 sequences...\n",
      "Serialized 7300 sequences...\n",
      "Serialized 7400 sequences...\n",
      "Serialized 7500 sequences...\n",
      "Serialized 7600 sequences...\n",
      "Serialized 7700 sequences...\n",
      "Serialized 7800 sequences...\n",
      "Serialized 7900 sequences...\n",
      "Serialized 8000 sequences...\n",
      "Serialized 8100 sequences...\n",
      "Serialized 8200 sequences...\n",
      "Serialized 8300 sequences...\n",
      "Serialized 8400 sequences...\n",
      "Serialized 8500 sequences...\n",
      "Serialized 8600 sequences...\n",
      "Serialized 8700 sequences...\n",
      "Serialized 8800 sequences...\n",
      "Serialized 8900 sequences...\n",
      "Serialized 9000 sequences...\n",
      "Serialized 9100 sequences...\n",
      "Serialized 9200 sequences...\n",
      "Serialized 9300 sequences...\n",
      "Serialized 9400 sequences...\n",
      "Serialized 9500 sequences...\n",
      "Serialized 9600 sequences...\n",
      "Serialized 9700 sequences...\n",
      "Serialized 9800 sequences...\n",
      "Serialized 9900 sequences...\n",
      "Serialized 10000 sequences...\n",
      "Serialized 10100 sequences...\n",
      "Serialized 10200 sequences...\n",
      "Serialized 10300 sequences...\n",
      "Serialized 10400 sequences...\n",
      "Serialized 10500 sequences...\n",
      "Serialized 10600 sequences...\n",
      "Serialized 10700 sequences...\n",
      "Serialized 10800 sequences...\n",
      "Serialized 10900 sequences...\n",
      "Serialized 11000 sequences...\n",
      "Serialized 11100 sequences...\n",
      "Serialized 11200 sequences...\n",
      "Serialized 11300 sequences...\n",
      "Serialized 11400 sequences...\n",
      "Serialized 11500 sequences...\n",
      "Serialized 11600 sequences...\n",
      "Serialized 11700 sequences...\n",
      "Serialized 11800 sequences...\n",
      "Serialized 11900 sequences...\n",
      "Serialized 12000 sequences...\n",
      "Serialized 12100 sequences...\n",
      "Serialized 12200 sequences...\n",
      "Serialized 12300 sequences...\n",
      "Serialized 12400 sequences...\n",
      "Serialized 12500 sequences...\n",
      "Serialized 12600 sequences...\n",
      "Serialized 12700 sequences...\n",
      "Serialized 12800 sequences...\n",
      "Serialized 12900 sequences...\n",
      "Serialized 13000 sequences...\n",
      "Serialized 13100 sequences...\n",
      "Serialized 13200 sequences...\n",
      "Serialized 13300 sequences...\n",
      "Serialized 13400 sequences...\n",
      "Serialized 13500 sequences...\n",
      "Serialized 13600 sequences...\n",
      "Serialized 13700 sequences...\n",
      "Serialized 13800 sequences...\n",
      "Serialized 13900 sequences...\n",
      "Serialized 14000 sequences...\n",
      "Serialized 14100 sequences...\n",
      "Serialized 14200 sequences...\n",
      "Serialized 14300 sequences...\n",
      "Serialized 14400 sequences...\n",
      "Serialized 14500 sequences...\n",
      "Serialized 14600 sequences...\n",
      "Serialized 14700 sequences...\n",
      "Serialized 14800 sequences...\n",
      "Serialized 14900 sequences...\n",
      "Serialized 15000 sequences...\n",
      "Serialized 15100 sequences...\n",
      "Serialized 15200 sequences...\n",
      "Serialized 15300 sequences...\n",
      "Serialized 15400 sequences...\n",
      "Serialized 15500 sequences...\n",
      "Serialized 15600 sequences...\n",
      "Serialized 15700 sequences...\n",
      "Serialized 15800 sequences...\n",
      "Serialized 15900 sequences...\n",
      "Serialized 16000 sequences...\n",
      "Serialized 16100 sequences...\n",
      "Serialized 16200 sequences...\n",
      "Serialized 16300 sequences...\n",
      "Serialized 16400 sequences...\n",
      "Serialized 16500 sequences...\n",
      "Serialized 16600 sequences...\n",
      "Serialized 16700 sequences...\n",
      "Serialized 16800 sequences...\n",
      "Serialized 16900 sequences...\n",
      "Serialized 17000 sequences...\n",
      "Serialized 17100 sequences...\n",
      "Serialized 17200 sequences...\n",
      "Serialized 17300 sequences...\n",
      "Serialized 17400 sequences...\n",
      "Serialized 17500 sequences...\n",
      "Serialized 17600 sequences...\n",
      "Serialized 17700 sequences...\n",
      "Serialized 17800 sequences...\n",
      "Serialized 17900 sequences...\n",
      "Serialized 18000 sequences...\n",
      "Serialized 18100 sequences...\n",
      "Serialized 18200 sequences...\n",
      "Serialized 18300 sequences...\n",
      "Serialized 18400 sequences...\n",
      "Serialized 18500 sequences...\n",
      "Serialized 18600 sequences...\n",
      "Serialized 18700 sequences...\n",
      "Serialized 18800 sequences...\n",
      "Serialized 18900 sequences...\n",
      "Serialized 19000 sequences...\n",
      "Serialized 19100 sequences...\n",
      "Serialized 19200 sequences...\n",
      "Serialized 19300 sequences...\n",
      "Serialized 19400 sequences...\n",
      "Serialized 19500 sequences...\n",
      "Serialized 19600 sequences...\n",
      "Serialized 19700 sequences...\n",
      "Serialized 19800 sequences...\n",
      "Serialized 19900 sequences...\n",
      "Serialized 20000 sequences...\n",
      "Serialized 20100 sequences...\n",
      "Serialized 20200 sequences...\n",
      "Serialized 20300 sequences...\n",
      "Serialized 20400 sequences...\n",
      "Serialized 20500 sequences...\n",
      "Serialized 20600 sequences...\n",
      "Serialized 20700 sequences...\n",
      "Serialized 20800 sequences...\n",
      "Serialized 20900 sequences...\n",
      "Serialized 21000 sequences...\n",
      "Serialized 21100 sequences...\n",
      "Serialized 21200 sequences...\n",
      "Serialized 21300 sequences...\n",
      "Serialized 21400 sequences...\n",
      "Serialized 21500 sequences...\n",
      "Serialized 21600 sequences...\n",
      "Serialized 21700 sequences...\n",
      "Serialized 21800 sequences...\n",
      "Serialized 21900 sequences...\n",
      "Serialized 22000 sequences...\n",
      "Serialized 22100 sequences...\n",
      "Serialized 22200 sequences...\n",
      "Serialized 22300 sequences...\n",
      "Serialized 22400 sequences...\n",
      "Serialized 22500 sequences...\n",
      "Serialized 22600 sequences...\n",
      "Serialized 22700 sequences...\n",
      "Serialized 22800 sequences...\n",
      "Serialized 22900 sequences...\n"
     ]
    }
   ],
   "source": [
    "!python prepare_data.py skp_vocab.txt skp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "zqH2KQhMT1ZX",
    "outputId": "08e91270-17b0-47f3-a11c-84a228ea7b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': 1, 'j': 2, 'i': 3, 'S': 4, 'P': 5, 'W': 6, 'b': 7, 'T': 8, 'V': 9, '.': 10, 'I': 11, 'D': 12, 'v': 13, 'K': 14, '!': 15, 'g': 16, 'p': 17, ']': 18, 'e': 19, '3': 20, 'm': 21, 'y': 22, 'G': 23, 'c': 24, '&': 25, 'h': 26, 'U': 27, 'Q': 28, 'l': 29, 'H': 30, 'n': 31, 'a': 32, 'F': 33, 'r': 34, '[': 35, 'z': 36, ':': 37, 'O': 38, '$': 39, 'u': 40, 'f': 41, ',': 42, 'R': 43, 'L': 44, 'd': 45, '?': 46, 'C': 47, 'Z': 48, ' ': 49, 'A': 50, \"'\": 51, 'E': 52, 't': 53, 'B': 54, 'x': 55, 'J': 56, 'Y': 57, '-': 58, 'q': 59, 'M': 60, 's': 61, 'X': 62, '\\n': 63, ';': 64, 'k': 65, 'N': 66, 'o': 67, '<S>': 0}\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "from prepare_data import parse_seq\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "filename = \"WandB.pickle\"\n",
    "\n",
    "\n",
    "# this is just a datasets of \"bytes\" (not understandable)\n",
    "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
    "\n",
    "# this maps a parser function that properly interprets the bytes over the dataset\n",
    "# (with fixed sequence length 200)\n",
    "# if you change the sequence length in preprocessing you also need to change it here\n",
    "sequenceLength = 200\n",
    "data = data.map(lambda x: parse_seq(x, sequenceLength))\n",
    "\n",
    "# a map from characters to indices\n",
    "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
    "vocab_size = len(vocab)\n",
    "# inverse mapping: indices to characters\n",
    "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
    "\n",
    "print(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qW-9h7nhgQry",
    "outputId": "7da6ce9a-be37-4785-805c-0d162632dcf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(64, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "batch_OneHot_data = data.shuffle(buffer_size = 10 * batch_size).batch(batch_size=batch_size, drop_remainder=True).repeat()\n",
    "batch_size = tf.convert_to_tensor(batch_size, dtype=None, dtype_hint=None, name=None)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZjqwMBKMWUu"
   },
   "outputs": [],
   "source": [
    "def initializer(hunits, vocab_size, sequenceLength):\n",
    "    \n",
    "    \n",
    "    # Input to hidden layer\n",
    "    W_InpToHidden = tf.Variable(tf.initializers.glorot_uniform()(shape=[vocab_size, hunits]))\n",
    "    b_InpToHidden = tf.Variable(tf.initializers.glorot_uniform()(shape=[hunits]))\n",
    "    \n",
    "    # hidden to hidden layer\n",
    "    W_HiddenToHidden = tf.Variable(tf.initializers.glorot_uniform()(shape=[hunits,hunits]))\n",
    "\n",
    "    \n",
    "    # hidden to output layer\n",
    "    W_HiddenToOutput = tf.Variable(tf.initializers.glorot_uniform()(shape=[hunits,vocab_size]))\n",
    "    b_HiddenToOutput = tf.Variable(tf.initializers.glorot_uniform()(shape=[vocab_size]))\n",
    "    \n",
    "    trainVariables = [W_InpToHidden, b_InpToHidden, W_HiddenToHidden, W_HiddenToOutput, b_HiddenToOutput]\n",
    "    return trainVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "Mvx_RSQaMMag",
    "outputId": "b012f569-742c-46ed-f60a-ae3fffb79262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 512)\n",
      "(512,)\n",
      "(512, 512)\n",
      "(512, 68)\n",
      "(68,)\n"
     ]
    }
   ],
   "source": [
    "units = 512\n",
    "trainVariables = initializer(units, vocab_size, sequenceLength)\n",
    "\n",
    "for value in trainVariables:\n",
    "    print(value.shape)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPOKYLHxWzDv"
   },
   "outputs": [],
   "source": [
    "\n",
    "opt = tf.optimizers.Adam()\n",
    "n_time_steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unvrgM6CXADK"
   },
   "outputs": [],
   "source": [
    "#All operations were executed using tensorflow as python operations do not work under tensorflow\n",
    "\n",
    "# @tf.function\n",
    "def RNNCell(seq):\n",
    "  with tf.GradientTape() as tape:\n",
    "    state = tf.zeros([batch_size, units])\n",
    "    \n",
    "\n",
    "\n",
    "    for i in tf.range(n_time_steps-1):\n",
    "        inp = tf.one_hot(seq[:, i], vocab_size)\n",
    "        # # print(\"inp:  \", inp.shape)\n",
    "        op_InpToHidden = tf.matmul(inp, trainVariables[0])\n",
    "        # # print(\"op_InpToHidden :  \", op_InpToHidden.shape) \n",
    "        op_HiddenToHidden = tf.matmul(state, trainVariables[2]) + trainVariables[1] \n",
    "        # # print(\"op_HiddenToHidden :  \", op_HiddenToHidden.shape)\n",
    "        op_FromHidden = op_InpToHidden + op_HiddenToHidden\n",
    "        # # print(\"op_FromHidden :  \", op_FromHidden.shape)\n",
    "        state = tf.nn.tanh(op_FromHidden)\n",
    "        # print(\"op_FromHidden :  \", op_FromHidden.shape)\n",
    "\n",
    "\n",
    "        # Second node\n",
    "        y = tf.matmul(state, trainVariables[3]) + trainVariables[4]\n",
    "        \n",
    "        # print(y)\n",
    "\n",
    "        labels = seqs[:, i+1]\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, y)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        train_perplexity = tf.exp(loss)\n",
    "\n",
    "        accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        acc = accuracy(labels, y)\n",
    "\n",
    "\n",
    "\n",
    "    grads = tape.gradient(loss, trainVariables)\n",
    "\n",
    "    \n",
    "    opt.apply_gradients(zip(grads, trainVariables))\n",
    "\n",
    "    return loss, acc, train_perplexity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    steps = 10000\n",
    "    \n",
    "    for step, seqs in enumerate(batch_OneHot_data):\n",
    "    \n",
    "        loss, acc= RNNCell(seqs)\n",
    "    \n",
    "        if not step % 100:\n",
    "            print(\"Step: {} Loss: {} Acc: {}\\n\".format(step, loss, acc))\n",
    "    \n",
    "        if step > steps:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wXHcTc-gc-Q4",
    "outputId": "21b11d7f-3f16-48e0-8f77-e83e6fea55b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 4.187745571136475 ACC: 0.015625 Perplexity: 65.87411499023438 \n",
      "\n",
      "Step: 100 Loss: 3.315243721008301 ACC: 0.171875 Perplexity: 27.529102325439453 \n",
      "\n",
      "Step: 200 Loss: 2.7459614276885986 ACC: 0.296875 Perplexity: 15.579585075378418 \n",
      "\n",
      "Step: 300 Loss: 2.98148250579834 ACC: 0.1875 Perplexity: 19.717025756835938 \n",
      "\n",
      "Step: 400 Loss: 2.7067885398864746 ACC: 0.3125 Perplexity: 14.981085777282715 \n",
      "\n",
      "Step: 500 Loss: 2.5962560176849365 ACC: 0.28125 Perplexity: 13.413423538208008 \n",
      "\n",
      "Step: 600 Loss: 2.499633312225342 ACC: 0.3125 Perplexity: 12.178028106689453 \n",
      "\n",
      "Step: 700 Loss: 2.532416820526123 ACC: 0.359375 Perplexity: 12.583882331848145 \n",
      "\n",
      "Step: 800 Loss: 2.3146772384643555 ACC: 0.3125 Perplexity: 10.121655464172363 \n",
      "\n",
      "Step: 900 Loss: 2.4612746238708496 ACC: 0.34375 Perplexity: 11.71973991394043 \n",
      "\n",
      "Step: 1000 Loss: 2.3218870162963867 ACC: 0.25 Perplexity: 10.194893836975098 \n",
      "\n",
      "Step: 1100 Loss: 2.4397318363189697 ACC: 0.265625 Perplexity: 11.469964981079102 \n",
      "\n",
      "Step: 1200 Loss: 2.607088088989258 ACC: 0.265625 Perplexity: 13.55950927734375 \n",
      "\n",
      "Step: 1300 Loss: 2.1486263275146484 ACC: 0.3125 Perplexity: 8.573073387145996 \n",
      "\n",
      "Step: 1400 Loss: 2.4336345195770264 ACC: 0.328125 Perplexity: 11.40024185180664 \n",
      "\n",
      "Step: 1500 Loss: 2.114084005355835 ACC: 0.375 Perplexity: 8.28199577331543 \n",
      "\n",
      "Step: 1600 Loss: 2.292940855026245 ACC: 0.34375 Perplexity: 9.904021263122559 \n",
      "\n",
      "Step: 1700 Loss: 2.1159157752990723 ACC: 0.421875 Perplexity: 8.29718017578125 \n",
      "\n",
      "Step: 1800 Loss: 2.2972474098205566 ACC: 0.359375 Perplexity: 9.946764945983887 \n",
      "\n",
      "Step: 1900 Loss: 2.437295436859131 ACC: 0.375 Perplexity: 11.442052841186523 \n",
      "\n",
      "Step: 2000 Loss: 2.481567621231079 ACC: 0.328125 Perplexity: 11.95999813079834 \n",
      "\n",
      "Step: 2100 Loss: 2.3248047828674316 ACC: 0.296875 Perplexity: 10.22468376159668 \n",
      "\n",
      "Step: 2200 Loss: 2.0503358840942383 ACC: 0.40625 Perplexity: 7.770510673522949 \n",
      "\n",
      "Step: 2300 Loss: 2.270573616027832 ACC: 0.296875 Perplexity: 9.684954643249512 \n",
      "\n",
      "Step: 2400 Loss: 2.251756429672241 ACC: 0.375 Perplexity: 9.504414558410645 \n",
      "\n",
      "Step: 2500 Loss: 2.352574348449707 ACC: 0.25 Perplexity: 10.512598037719727 \n",
      "\n",
      "Step: 2600 Loss: 2.0370726585388184 ACC: 0.453125 Perplexity: 7.6681294441223145 \n",
      "\n",
      "Step: 2700 Loss: 1.9252983331680298 ACC: 0.515625 Perplexity: 6.857193946838379 \n",
      "\n",
      "Step: 2800 Loss: 2.3497939109802246 ACC: 0.328125 Perplexity: 10.48340892791748 \n",
      "\n",
      "Step: 2900 Loss: 2.3264989852905273 ACC: 0.359375 Perplexity: 10.242021560668945 \n",
      "\n",
      "Step: 3000 Loss: 2.097208023071289 ACC: 0.359375 Perplexity: 8.143402099609375 \n",
      "\n",
      "Step: 3100 Loss: 1.961047887802124 ACC: 0.359375 Perplexity: 7.1067705154418945 \n",
      "\n",
      "Step: 3200 Loss: 2.046424627304077 ACC: 0.390625 Perplexity: 7.740177154541016 \n",
      "\n",
      "Step: 3300 Loss: 2.2957029342651367 ACC: 0.328125 Perplexity: 9.931415557861328 \n",
      "\n",
      "Step: 3400 Loss: 2.04864764213562 ACC: 0.421875 Perplexity: 7.757403373718262 \n",
      "\n",
      "Step: 3500 Loss: 2.3319292068481445 ACC: 0.3125 Perplexity: 10.297789573669434 \n",
      "\n",
      "Step: 3600 Loss: 2.2621352672576904 ACC: 0.328125 Perplexity: 9.6035737991333 \n",
      "\n",
      "Step: 3700 Loss: 2.3609530925750732 ACC: 0.390625 Perplexity: 10.601049423217773 \n",
      "\n",
      "Step: 3800 Loss: 1.9605705738067627 ACC: 0.4375 Perplexity: 7.103378772735596 \n",
      "\n",
      "Step: 3900 Loss: 1.9912900924682617 ACC: 0.359375 Perplexity: 7.324977397918701 \n",
      "\n",
      "Step: 4000 Loss: 2.2331717014312744 ACC: 0.375 Perplexity: 9.329408645629883 \n",
      "\n",
      "Step: 4100 Loss: 2.1265578269958496 ACC: 0.359375 Perplexity: 8.385951042175293 \n",
      "\n",
      "Step: 4200 Loss: 1.8974285125732422 ACC: 0.4375 Perplexity: 6.6687235832214355 \n",
      "\n",
      "Step: 4300 Loss: 2.0322790145874023 ACC: 0.421875 Perplexity: 7.631458759307861 \n",
      "\n",
      "Step: 4400 Loss: 2.028704881668091 ACC: 0.4375 Perplexity: 7.604231357574463 \n",
      "\n",
      "Step: 4500 Loss: 2.1645712852478027 ACC: 0.34375 Perplexity: 8.710866928100586 \n",
      "\n",
      "Step: 4600 Loss: 2.188373565673828 ACC: 0.375 Perplexity: 8.920692443847656 \n",
      "\n",
      "Step: 4700 Loss: 1.9598790407180786 ACC: 0.375 Perplexity: 7.09846830368042 \n",
      "\n",
      "Step: 4800 Loss: 2.0108187198638916 ACC: 0.34375 Perplexity: 7.469429969787598 \n",
      "\n",
      "Step: 4900 Loss: 2.177502155303955 ACC: 0.359375 Perplexity: 8.824236869812012 \n",
      "\n",
      "Step: 5000 Loss: 2.2396399974823 ACC: 0.359375 Perplexity: 9.389949798583984 \n",
      "\n",
      "Step: 5100 Loss: 1.9602411985397339 ACC: 0.421875 Perplexity: 7.101039409637451 \n",
      "\n",
      "Step: 5200 Loss: 2.035007953643799 ACC: 0.390625 Perplexity: 7.652312755584717 \n",
      "\n",
      "Step: 5300 Loss: 2.1411375999450684 ACC: 0.390625 Perplexity: 8.509112358093262 \n",
      "\n",
      "Step: 5400 Loss: 1.8285914659500122 ACC: 0.421875 Perplexity: 6.225112438201904 \n",
      "\n",
      "Step: 5500 Loss: 2.1279726028442383 ACC: 0.328125 Perplexity: 8.397823333740234 \n",
      "\n",
      "Step: 5600 Loss: 2.4076614379882812 ACC: 0.3125 Perplexity: 11.107953071594238 \n",
      "\n",
      "Step: 5700 Loss: 2.2813754081726074 ACC: 0.359375 Perplexity: 9.790136337280273 \n",
      "\n",
      "Step: 5800 Loss: 1.8147859573364258 ACC: 0.46875 Perplexity: 6.139761924743652 \n",
      "\n",
      "Step: 5900 Loss: 1.6967982053756714 ACC: 0.46875 Perplexity: 5.456449031829834 \n",
      "\n",
      "Step: 6000 Loss: 1.9835647344589233 ACC: 0.40625 Perplexity: 7.2686076164245605 \n",
      "\n",
      "Step: 6100 Loss: 1.947064757347107 ACC: 0.46875 Perplexity: 7.008087158203125 \n",
      "\n",
      "Step: 6200 Loss: 1.9557355642318726 ACC: 0.484375 Perplexity: 7.069117069244385 \n",
      "\n",
      "Step: 6300 Loss: 2.2002038955688477 ACC: 0.375 Perplexity: 9.026853561401367 \n",
      "\n",
      "Step: 6400 Loss: 1.6171867847442627 ACC: 0.515625 Perplexity: 5.038895130157471 \n",
      "\n",
      "Step: 6500 Loss: 2.181655168533325 ACC: 0.359375 Perplexity: 8.860960960388184 \n",
      "\n",
      "Step: 6600 Loss: 2.0126943588256836 ACC: 0.375 Perplexity: 7.483453273773193 \n",
      "\n",
      "Step: 6700 Loss: 2.0753860473632812 ACC: 0.328125 Perplexity: 7.967621326446533 \n",
      "\n",
      "Step: 6800 Loss: 1.4577949047088623 ACC: 0.640625 Perplexity: 4.296474933624268 \n",
      "\n",
      "Step: 6900 Loss: 1.6194953918457031 ACC: 0.515625 Perplexity: 5.050541400909424 \n",
      "\n",
      "Step: 7000 Loss: 2.2147467136383057 ACC: 0.296875 Perplexity: 9.159089088439941 \n",
      "\n",
      "Step: 7100 Loss: 2.034329652786255 ACC: 0.46875 Perplexity: 7.64712381362915 \n",
      "\n",
      "Step: 7200 Loss: 2.218413829803467 ACC: 0.34375 Perplexity: 9.192737579345703 \n",
      "\n",
      "Step: 7300 Loss: 1.7950310707092285 ACC: 0.40625 Perplexity: 6.0196614265441895 \n",
      "\n",
      "Step: 7400 Loss: 1.7116672992706299 ACC: 0.453125 Perplexity: 5.538187503814697 \n",
      "\n",
      "Step: 7500 Loss: 1.915217399597168 ACC: 0.46875 Perplexity: 6.788414478302002 \n",
      "\n",
      "Step: 7600 Loss: 1.9396308660507202 ACC: 0.375 Perplexity: 6.956182479858398 \n",
      "\n",
      "Step: 7700 Loss: 1.8587793111801147 ACC: 0.453125 Perplexity: 6.415900230407715 \n",
      "\n",
      "Step: 7800 Loss: 1.994296908378601 ACC: 0.4375 Perplexity: 7.3470354080200195 \n",
      "\n",
      "Step: 7900 Loss: 1.9153473377227783 ACC: 0.328125 Perplexity: 6.789296627044678 \n",
      "\n",
      "Step: 8000 Loss: 2.1301722526550293 ACC: 0.4375 Perplexity: 8.416316986083984 \n",
      "\n",
      "Step: 8100 Loss: 1.8354616165161133 ACC: 0.390625 Perplexity: 6.268026828765869 \n",
      "\n",
      "Step: 8200 Loss: 1.9439551830291748 ACC: 0.421875 Perplexity: 6.986328601837158 \n",
      "\n",
      "Step: 8300 Loss: 2.126811981201172 ACC: 0.4375 Perplexity: 8.388082504272461 \n",
      "\n",
      "Step: 8400 Loss: 1.8464891910552979 ACC: 0.4375 Perplexity: 6.337530612945557 \n",
      "\n",
      "Step: 8500 Loss: 1.684626579284668 ACC: 0.453125 Perplexity: 5.390438079833984 \n",
      "\n",
      "Step: 8600 Loss: 1.9213125705718994 ACC: 0.4375 Perplexity: 6.8299174308776855 \n",
      "\n",
      "Step: 8700 Loss: 1.783682107925415 ACC: 0.484375 Perplexity: 5.951731204986572 \n",
      "\n",
      "Step: 8800 Loss: 1.860410451889038 ACC: 0.421875 Perplexity: 6.4263739585876465 \n",
      "\n",
      "Step: 8900 Loss: 1.4449163675308228 ACC: 0.59375 Perplexity: 4.24149751663208 \n",
      "\n",
      "Step: 9000 Loss: 1.9116573333740234 ACC: 0.421875 Perplexity: 6.7642903327941895 \n",
      "\n",
      "Step: 9100 Loss: 1.468887209892273 ACC: 0.5625 Perplexity: 4.344398021697998 \n",
      "\n",
      "Step: 9200 Loss: 1.721560001373291 ACC: 0.515625 Perplexity: 5.593246936798096 \n",
      "\n",
      "Step: 9300 Loss: 1.5439352989196777 ACC: 0.546875 Perplexity: 4.682982921600342 \n",
      "\n",
      "Step: 9400 Loss: 2.140014886856079 ACC: 0.421875 Perplexity: 8.499564170837402 \n",
      "\n",
      "Step: 9500 Loss: 1.6959869861602783 ACC: 0.453125 Perplexity: 5.452024459838867 \n",
      "\n",
      "Step: 9600 Loss: 1.7179901599884033 ACC: 0.5 Perplexity: 5.573315143585205 \n",
      "\n",
      "Step: 9700 Loss: 1.8370227813720703 ACC: 0.421875 Perplexity: 6.277820110321045 \n",
      "\n",
      "Step: 9800 Loss: 2.1322250366210938 ACC: 0.3125 Perplexity: 8.433610916137695 \n",
      "\n",
      "Step: 9900 Loss: 1.6016404628753662 ACC: 0.453125 Perplexity: 4.961164474487305 \n",
      "\n",
      "Step: 10000 Loss: 1.6669732332229614 ACC: 0.59375 Perplexity: 5.29611349105835 \n",
      "\n",
      "Step: 10100 Loss: 1.6623315811157227 ACC: 0.4375 Perplexity: 5.27158784866333 \n",
      "\n",
      "Step: 10200 Loss: 1.4804370403289795 ACC: 0.578125 Perplexity: 4.394865989685059 \n",
      "\n",
      "Step: 10300 Loss: 1.8604446649551392 ACC: 0.453125 Perplexity: 6.426593780517578 \n",
      "\n",
      "Step: 10400 Loss: 1.8936259746551514 ACC: 0.4375 Perplexity: 6.64341402053833 \n",
      "\n",
      "Step: 10500 Loss: 1.9888709783554077 ACC: 0.484375 Perplexity: 7.307279109954834 \n",
      "\n",
      "Step: 10600 Loss: 1.730340838432312 ACC: 0.515625 Perplexity: 5.642576694488525 \n",
      "\n",
      "Step: 10700 Loss: 1.4916279315948486 ACC: 0.53125 Perplexity: 4.444324970245361 \n",
      "\n",
      "Step: 10800 Loss: 1.9864871501922607 ACC: 0.375 Perplexity: 7.289880752563477 \n",
      "\n",
      "Step: 10900 Loss: 1.897404432296753 ACC: 0.40625 Perplexity: 6.668563365936279 \n",
      "\n",
      "Step: 11000 Loss: 1.4554517269134521 ACC: 0.515625 Perplexity: 4.28641939163208 \n",
      "\n",
      "Step: 11100 Loss: 1.4768595695495605 ACC: 0.546875 Perplexity: 4.379171371459961 \n",
      "\n",
      "Step: 11200 Loss: 2.0780487060546875 ACC: 0.34375 Perplexity: 7.988864898681641 \n",
      "\n",
      "Step: 11300 Loss: 2.0253539085388184 ACC: 0.390625 Perplexity: 7.578792572021484 \n",
      "\n",
      "Step: 11400 Loss: 1.5407004356384277 ACC: 0.5 Perplexity: 4.667858600616455 \n",
      "\n",
      "Step: 11500 Loss: 1.9513615369796753 ACC: 0.34375 Perplexity: 7.03826379776001 \n",
      "\n",
      "Step: 11600 Loss: 1.7710583209991455 ACC: 0.5 Perplexity: 5.87706995010376 \n",
      "\n",
      "Step: 11700 Loss: 1.7902131080627441 ACC: 0.46875 Perplexity: 5.990728855133057 \n",
      "\n",
      "Step: 11800 Loss: 1.725143313407898 ACC: 0.484375 Perplexity: 5.613325119018555 \n",
      "\n",
      "Step: 11900 Loss: 1.394624948501587 ACC: 0.65625 Perplexity: 4.033461570739746 \n",
      "\n",
      "Step: 12000 Loss: 1.74686598777771 ACC: 0.46875 Perplexity: 5.73659610748291 \n",
      "\n",
      "Step: 12100 Loss: 1.7912068367004395 ACC: 0.421875 Perplexity: 5.996685028076172 \n",
      "\n",
      "Step: 12200 Loss: 1.965070128440857 ACC: 0.34375 Perplexity: 7.13541316986084 \n",
      "\n",
      "Step: 12300 Loss: 1.692650318145752 ACC: 0.46875 Perplexity: 5.433863162994385 \n",
      "\n",
      "Step: 12400 Loss: 1.5896868705749512 ACC: 0.515625 Perplexity: 4.9022135734558105 \n",
      "\n",
      "Step: 12500 Loss: 1.5107816457748413 ACC: 0.453125 Perplexity: 4.530270576477051 \n",
      "\n",
      "Step: 12600 Loss: 1.7808609008789062 ACC: 0.46875 Perplexity: 5.934963703155518 \n",
      "\n",
      "Step: 12700 Loss: 1.7365528345108032 ACC: 0.5 Perplexity: 5.677737712860107 \n",
      "\n",
      "Step: 12800 Loss: 1.7768900394439697 ACC: 0.421875 Perplexity: 5.91144323348999 \n",
      "\n",
      "Step: 12900 Loss: 1.8525128364562988 ACC: 0.453125 Perplexity: 6.375820636749268 \n",
      "\n",
      "Step: 13000 Loss: 1.5635461807250977 ACC: 0.484375 Perplexity: 4.775726795196533 \n",
      "\n",
      "Step: 13100 Loss: 1.8729203939437866 ACC: 0.53125 Perplexity: 6.507272720336914 \n",
      "\n",
      "Step: 13200 Loss: 1.9974937438964844 ACC: 0.359375 Perplexity: 7.370560169219971 \n",
      "\n",
      "Step: 13300 Loss: 1.6785765886306763 ACC: 0.484375 Perplexity: 5.35792350769043 \n",
      "\n",
      "Step: 13400 Loss: 1.8415043354034424 ACC: 0.375 Perplexity: 6.3060173988342285 \n",
      "\n",
      "Step: 13500 Loss: 1.5805147886276245 ACC: 0.5 Perplexity: 4.857455730438232 \n",
      "\n",
      "Step: 13600 Loss: 1.9881975650787354 ACC: 0.421875 Perplexity: 7.302359580993652 \n",
      "\n",
      "Step: 13700 Loss: 1.7330422401428223 ACC: 0.515625 Perplexity: 5.657840251922607 \n",
      "\n",
      "Step: 13800 Loss: 1.6298580169677734 ACC: 0.46875 Perplexity: 5.103149890899658 \n",
      "\n",
      "Step: 13900 Loss: 1.7312719821929932 ACC: 0.453125 Perplexity: 5.647833347320557 \n",
      "\n",
      "Step: 14000 Loss: 1.9537450075149536 ACC: 0.484375 Perplexity: 7.055059432983398 \n",
      "\n",
      "Step: 14100 Loss: 1.6329078674316406 ACC: 0.546875 Perplexity: 5.118737697601318 \n",
      "\n",
      "Step: 14200 Loss: 1.9544498920440674 ACC: 0.421875 Perplexity: 7.060033798217773 \n",
      "\n",
      "Step: 14300 Loss: 2.0735464096069336 ACC: 0.390625 Perplexity: 7.952977657318115 \n",
      "\n",
      "Step: 14400 Loss: 1.8329262733459473 ACC: 0.46875 Perplexity: 6.252155303955078 \n",
      "\n",
      "Step: 14500 Loss: 2.023977279663086 ACC: 0.390625 Perplexity: 7.568366527557373 \n",
      "\n",
      "Step: 14600 Loss: 1.89066743850708 ACC: 0.40625 Perplexity: 6.623788356781006 \n",
      "\n",
      "Step: 14700 Loss: 1.8869636058807373 ACC: 0.375 Perplexity: 6.599300384521484 \n",
      "\n",
      "Step: 14800 Loss: 1.6230041980743408 ACC: 0.546875 Perplexity: 5.068293571472168 \n",
      "\n",
      "Step: 14900 Loss: 1.6946537494659424 ACC: 0.46875 Perplexity: 5.444760322570801 \n",
      "\n",
      "Step: 15000 Loss: 1.731592059135437 ACC: 0.46875 Perplexity: 5.649641036987305 \n",
      "\n",
      "Step: 15100 Loss: 1.824220895767212 ACC: 0.359375 Perplexity: 6.197964668273926 \n",
      "\n",
      "Step: 15200 Loss: 1.8605705499649048 ACC: 0.515625 Perplexity: 6.427402973175049 \n",
      "\n",
      "Step: 15300 Loss: 1.9493458271026611 ACC: 0.375 Perplexity: 7.0240912437438965 \n",
      "\n",
      "Step: 15400 Loss: 1.9433414936065674 ACC: 0.421875 Perplexity: 6.98204231262207 \n",
      "\n",
      "Step: 15500 Loss: 1.8579871654510498 ACC: 0.40625 Perplexity: 6.410820007324219 \n",
      "\n",
      "Step: 15600 Loss: 1.8740620613098145 ACC: 0.46875 Perplexity: 6.514705657958984 \n",
      "\n",
      "Step: 15700 Loss: 1.964172601699829 ACC: 0.40625 Perplexity: 7.129011631011963 \n",
      "\n",
      "Step: 15800 Loss: 1.6413861513137817 ACC: 0.5 Perplexity: 5.162320137023926 \n",
      "\n",
      "Step: 15900 Loss: 1.6282968521118164 ACC: 0.46875 Perplexity: 5.095189571380615 \n",
      "\n",
      "Step: 16000 Loss: 1.801464557647705 ACC: 0.453125 Perplexity: 6.05851411819458 \n",
      "\n",
      "Step: 16100 Loss: 2.0899147987365723 ACC: 0.375 Perplexity: 8.084226608276367 \n",
      "\n",
      "Step: 16200 Loss: 1.788243293762207 ACC: 0.46875 Perplexity: 5.978940010070801 \n",
      "\n",
      "Step: 16300 Loss: 1.7970070838928223 ACC: 0.453125 Perplexity: 6.0315680503845215 \n",
      "\n",
      "Step: 16400 Loss: 1.9342141151428223 ACC: 0.453125 Perplexity: 6.918604850769043 \n",
      "\n",
      "Step: 16500 Loss: 2.2934036254882812 ACC: 0.28125 Perplexity: 9.908605575561523 \n",
      "\n",
      "Step: 16600 Loss: 2.2436695098876953 ACC: 0.296875 Perplexity: 9.427864074707031 \n",
      "\n",
      "Step: 16700 Loss: 2.0409128665924072 ACC: 0.390625 Perplexity: 7.697633266448975 \n",
      "\n",
      "Step: 16800 Loss: 1.8748036623001099 ACC: 0.390625 Perplexity: 6.519538402557373 \n",
      "\n",
      "Step: 16900 Loss: 2.0196738243103027 ACC: 0.390625 Perplexity: 7.535866737365723 \n",
      "\n",
      "Step: 17000 Loss: 1.5499873161315918 ACC: 0.546875 Perplexity: 4.7114105224609375 \n",
      "\n",
      "Step: 17100 Loss: 1.9870331287384033 ACC: 0.34375 Perplexity: 7.293861389160156 \n",
      "\n",
      "Step: 17200 Loss: 1.6782041788101196 ACC: 0.453125 Perplexity: 5.355928897857666 \n",
      "\n",
      "Step: 17300 Loss: 1.697403907775879 ACC: 0.46875 Perplexity: 5.459754943847656 \n",
      "\n",
      "Step: 17400 Loss: 2.0410335063934326 ACC: 0.421875 Perplexity: 7.698561668395996 \n",
      "\n",
      "Step: 17500 Loss: 2.044114351272583 ACC: 0.453125 Perplexity: 7.722316265106201 \n",
      "\n",
      "Step: 17600 Loss: 2.2467284202575684 ACC: 0.390625 Perplexity: 9.456746101379395 \n",
      "\n",
      "Step: 17700 Loss: 1.879256248474121 ACC: 0.390625 Perplexity: 6.548632621765137 \n",
      "\n",
      "Step: 17800 Loss: 1.5268570184707642 ACC: 0.546875 Perplexity: 4.603684902191162 \n",
      "\n",
      "Step: 17900 Loss: 2.0436112880706787 ACC: 0.375 Perplexity: 7.718432903289795 \n",
      "\n",
      "Step: 18000 Loss: 1.946865200996399 ACC: 0.40625 Perplexity: 7.006688594818115 \n",
      "\n",
      "Step: 18100 Loss: 1.9244251251220703 ACC: 0.390625 Perplexity: 6.851208686828613 \n",
      "\n",
      "Step: 18200 Loss: 2.114025592803955 ACC: 0.359375 Perplexity: 8.281512260437012 \n",
      "\n",
      "Step: 18300 Loss: 2.2826850414276123 ACC: 0.34375 Perplexity: 9.802967071533203 \n",
      "\n",
      "Step: 18400 Loss: 1.748748779296875 ACC: 0.484375 Perplexity: 5.74740743637085 \n",
      "\n",
      "Step: 18500 Loss: 1.595370888710022 ACC: 0.53125 Perplexity: 4.93015718460083 \n",
      "\n",
      "Step: 18600 Loss: 1.4824204444885254 ACC: 0.609375 Perplexity: 4.403591632843018 \n",
      "\n",
      "Step: 18700 Loss: 2.0930514335632324 ACC: 0.390625 Perplexity: 8.109622955322266 \n",
      "\n",
      "Step: 18800 Loss: 2.438076972961426 ACC: 0.21875 Perplexity: 11.450998306274414 \n",
      "\n",
      "Step: 18900 Loss: 1.802013635635376 ACC: 0.421875 Perplexity: 6.0618414878845215 \n",
      "\n",
      "Step: 19000 Loss: 1.7558214664459229 ACC: 0.453125 Perplexity: 5.788200855255127 \n",
      "\n",
      "Step: 19100 Loss: 1.8744704723358154 ACC: 0.4375 Perplexity: 6.517366886138916 \n",
      "\n",
      "Step: 19200 Loss: 2.139335870742798 ACC: 0.40625 Perplexity: 8.493795394897461 \n",
      "\n",
      "Step: 19300 Loss: 1.8924825191497803 ACC: 0.390625 Perplexity: 6.635822296142578 \n",
      "\n",
      "Step: 19400 Loss: 1.9652900695800781 ACC: 0.359375 Perplexity: 7.136982440948486 \n",
      "\n",
      "Step: 19500 Loss: 2.1261258125305176 ACC: 0.375 Perplexity: 8.382328987121582 \n",
      "\n",
      "Step: 19600 Loss: 1.8245131969451904 ACC: 0.4375 Perplexity: 6.1997761726379395 \n",
      "\n",
      "Step: 19700 Loss: 2.043607711791992 ACC: 0.4375 Perplexity: 7.718404769897461 \n",
      "\n",
      "Step: 19800 Loss: 2.059246063232422 ACC: 0.25 Perplexity: 7.840056896209717 \n",
      "\n",
      "Step: 19900 Loss: 2.202211856842041 ACC: 0.390625 Perplexity: 9.044997215270996 \n",
      "\n",
      "Step: 20000 Loss: 1.7172292470932007 ACC: 0.546875 Perplexity: 5.5690765380859375 \n",
      "\n",
      "Step: 20100 Loss: 2.0513916015625 ACC: 0.328125 Perplexity: 7.7787184715271 \n",
      "\n",
      "Step: 20200 Loss: 1.6460142135620117 ACC: 0.546875 Perplexity: 5.186267375946045 \n",
      "\n",
      "Step: 20300 Loss: 2.1882429122924805 ACC: 0.3125 Perplexity: 8.919527053833008 \n",
      "\n",
      "Step: 20400 Loss: 2.291262626647949 ACC: 0.3125 Perplexity: 9.88741397857666 \n",
      "\n",
      "Step: 20500 Loss: 1.876495599746704 ACC: 0.46875 Perplexity: 6.530579090118408 \n",
      "\n",
      "Step: 20600 Loss: 1.784444808959961 ACC: 0.421875 Perplexity: 5.956272125244141 \n",
      "\n",
      "Step: 20700 Loss: 1.947274923324585 ACC: 0.4375 Perplexity: 7.009559631347656 \n",
      "\n",
      "Step: 20800 Loss: 1.8733317852020264 ACC: 0.359375 Perplexity: 6.509949684143066 \n",
      "\n",
      "Step: 20900 Loss: 2.044832706451416 ACC: 0.46875 Perplexity: 7.727865219116211 \n",
      "\n",
      "Step: 21000 Loss: 2.12906551361084 ACC: 0.421875 Perplexity: 8.407007217407227 \n",
      "\n",
      "Step: 21100 Loss: 2.00121808052063 ACC: 0.421875 Perplexity: 7.398061752319336 \n",
      "\n",
      "Step: 21200 Loss: 2.0061028003692627 ACC: 0.421875 Perplexity: 7.4342875480651855 \n",
      "\n",
      "Step: 21300 Loss: 2.2537600994110107 ACC: 0.359375 Perplexity: 9.523477554321289 \n",
      "\n",
      "Step: 21400 Loss: 1.962845802307129 ACC: 0.421875 Perplexity: 7.119558811187744 \n",
      "\n",
      "Step: 21500 Loss: 2.0078694820404053 ACC: 0.328125 Perplexity: 7.4474334716796875 \n",
      "\n",
      "Step: 21600 Loss: 2.104954481124878 ACC: 0.359375 Perplexity: 8.2067289352417 \n",
      "\n",
      "Step: 21700 Loss: 2.050042152404785 ACC: 0.328125 Perplexity: 7.768228054046631 \n",
      "\n",
      "Step: 21800 Loss: 2.0198235511779785 ACC: 0.40625 Perplexity: 7.5369954109191895 \n",
      "\n",
      "Step: 21900 Loss: 1.878185510635376 ACC: 0.40625 Perplexity: 6.541624069213867 \n",
      "\n",
      "Step: 22000 Loss: 2.2970926761627197 ACC: 0.296875 Perplexity: 9.945226669311523 \n",
      "\n",
      "Step: 22100 Loss: 2.065063714981079 ACC: 0.390625 Perplexity: 7.885799884796143 \n",
      "\n",
      "Step: 22200 Loss: 1.9739289283752441 ACC: 0.390625 Perplexity: 7.198904991149902 \n",
      "\n",
      "Step: 22300 Loss: 2.167654037475586 ACC: 0.40625 Perplexity: 8.737761497497559 \n",
      "\n",
      "Step: 22400 Loss: 1.9874682426452637 ACC: 0.40625 Perplexity: 7.297036170959473 \n",
      "\n",
      "Step: 22500 Loss: 1.9646786451339722 ACC: 0.453125 Perplexity: 7.132620334625244 \n",
      "\n",
      "Step: 22600 Loss: 1.837805986404419 ACC: 0.453125 Perplexity: 6.28273868560791 \n",
      "\n",
      "Step: 22700 Loss: 2.0872650146484375 ACC: 0.4375 Perplexity: 8.062833786010742 \n",
      "\n",
      "Step: 22800 Loss: 1.7601984739303589 ACC: 0.5 Perplexity: 5.813591003417969 \n",
      "\n",
      "Step: 22900 Loss: 1.9470508098602295 ACC: 0.40625 Perplexity: 7.007989406585693 \n",
      "\n",
      "Step: 23000 Loss: 2.1036734580993652 ACC: 0.390625 Perplexity: 8.196223258972168 \n",
      "\n",
      "Step: 23100 Loss: 2.097476005554199 ACC: 0.375 Perplexity: 8.145584106445312 \n",
      "\n",
      "Step: 23200 Loss: 2.0004072189331055 ACC: 0.453125 Perplexity: 7.392065525054932 \n",
      "\n",
      "Step: 23300 Loss: 1.9658682346343994 ACC: 0.40625 Perplexity: 7.141109943389893 \n",
      "\n",
      "Step: 23400 Loss: 2.1879305839538574 ACC: 0.390625 Perplexity: 8.916741371154785 \n",
      "\n",
      "Step: 23500 Loss: 2.50146746635437 ACC: 0.3125 Perplexity: 12.200384140014648 \n",
      "\n",
      "Step: 23600 Loss: 2.074573278427124 ACC: 0.359375 Perplexity: 7.961148262023926 \n",
      "\n",
      "Step: 23700 Loss: 2.1981523036956787 ACC: 0.390625 Perplexity: 9.008353233337402 \n",
      "\n",
      "Step: 23800 Loss: 2.076131820678711 ACC: 0.484375 Perplexity: 7.973566055297852 \n",
      "\n",
      "Step: 23900 Loss: 2.37280535697937 ACC: 0.328125 Perplexity: 10.727444648742676 \n",
      "\n",
      "Step: 24000 Loss: 1.9653971195220947 ACC: 0.40625 Perplexity: 7.137746810913086 \n",
      "\n",
      "Step: 24100 Loss: 1.9090263843536377 ACC: 0.453125 Perplexity: 6.746517181396484 \n",
      "\n",
      "Step: 24200 Loss: 2.229586601257324 ACC: 0.28125 Perplexity: 9.296021461486816 \n",
      "\n",
      "Step: 24300 Loss: 2.4069743156433105 ACC: 0.375 Perplexity: 11.100323677062988 \n",
      "\n",
      "Step: 24400 Loss: 1.797717571258545 ACC: 0.53125 Perplexity: 6.035855293273926 \n",
      "\n",
      "Step: 24500 Loss: 2.213498592376709 ACC: 0.34375 Perplexity: 9.147664070129395 \n",
      "\n",
      "Step: 24600 Loss: 2.279759407043457 ACC: 0.34375 Perplexity: 9.774328231811523 \n",
      "\n",
      "Step: 24700 Loss: 1.9736378192901611 ACC: 0.484375 Perplexity: 7.1968092918396 \n",
      "\n",
      "Step: 24800 Loss: 2.1591665744781494 ACC: 0.3125 Perplexity: 8.66391372680664 \n",
      "\n",
      "Step: 24900 Loss: 2.058340549468994 ACC: 0.375 Perplexity: 7.832960605621338 \n",
      "\n",
      "Step: 25000 Loss: 2.2557740211486816 ACC: 0.34375 Perplexity: 9.54267692565918 \n",
      "\n",
      "Step: 25100 Loss: 2.1094272136688232 ACC: 0.390625 Perplexity: 8.243517875671387 \n",
      "\n",
      "Step: 25200 Loss: 1.924666166305542 ACC: 0.4375 Perplexity: 6.852860927581787 \n",
      "\n",
      "Step: 25300 Loss: 2.543187141418457 ACC: 0.28125 Perplexity: 12.720147132873535 \n",
      "\n",
      "Step: 25400 Loss: 2.1263325214385986 ACC: 0.359375 Perplexity: 8.384061813354492 \n",
      "\n",
      "Step: 25500 Loss: 2.109729766845703 ACC: 0.375 Perplexity: 8.246012687683105 \n",
      "\n",
      "Step: 25600 Loss: 2.281465768814087 ACC: 0.421875 Perplexity: 9.791021347045898 \n",
      "\n",
      "Step: 25700 Loss: 2.073005199432373 ACC: 0.390625 Perplexity: 7.94867467880249 \n",
      "\n",
      "Step: 25800 Loss: 2.222538709640503 ACC: 0.3125 Perplexity: 9.230734825134277 \n",
      "\n",
      "Step: 25900 Loss: 2.2073915004730225 ACC: 0.375 Perplexity: 9.09196949005127 \n",
      "\n",
      "Step: 26000 Loss: 2.0987963676452637 ACC: 0.46875 Perplexity: 8.156347274780273 \n",
      "\n",
      "Step: 26100 Loss: 2.04034161567688 ACC: 0.421875 Perplexity: 7.693236827850342 \n",
      "\n",
      "Step: 26200 Loss: 2.122044563293457 ACC: 0.40625 Perplexity: 8.348188400268555 \n",
      "\n",
      "Step: 26300 Loss: 2.1772637367248535 ACC: 0.375 Perplexity: 8.822134017944336 \n",
      "\n",
      "Step: 26400 Loss: 1.822048306465149 ACC: 0.421875 Perplexity: 6.184513092041016 \n",
      "\n",
      "Step: 26500 Loss: 2.156930923461914 ACC: 0.40625 Perplexity: 8.644566535949707 \n",
      "\n",
      "Step: 26600 Loss: 2.0306339263916016 ACC: 0.46875 Perplexity: 7.618914604187012 \n",
      "\n",
      "Step: 26700 Loss: 1.9304139614105225 ACC: 0.40625 Perplexity: 6.89236307144165 \n",
      "\n",
      "Step: 26800 Loss: 2.014195442199707 ACC: 0.40625 Perplexity: 7.494694709777832 \n",
      "\n",
      "Step: 26900 Loss: 2.1269845962524414 ACC: 0.40625 Perplexity: 8.389530181884766 \n",
      "\n",
      "Step: 27000 Loss: 1.863383173942566 ACC: 0.390625 Perplexity: 6.4455060958862305 \n",
      "\n",
      "Step: 27100 Loss: 2.49349308013916 ACC: 0.25 Perplexity: 12.103480339050293 \n",
      "\n",
      "Step: 27200 Loss: 2.026036262512207 ACC: 0.46875 Perplexity: 7.58396577835083 \n",
      "\n",
      "Step: 27300 Loss: 2.3273258209228516 ACC: 0.265625 Perplexity: 10.250493049621582 \n",
      "\n",
      "Step: 27400 Loss: 2.199384927749634 ACC: 0.390625 Perplexity: 9.019464492797852 \n",
      "\n",
      "Step: 27500 Loss: 2.2661232948303223 ACC: 0.28125 Perplexity: 9.641948699951172 \n",
      "\n",
      "Step: 27600 Loss: 2.462616205215454 ACC: 0.328125 Perplexity: 11.7354736328125 \n",
      "\n",
      "Step: 27700 Loss: 2.0680809020996094 ACC: 0.546875 Perplexity: 7.9096293449401855 \n",
      "\n",
      "Step: 27800 Loss: 2.1193771362304688 ACC: 0.375 Perplexity: 8.325949668884277 \n",
      "\n",
      "Step: 27900 Loss: 2.491814613342285 ACC: 0.265625 Perplexity: 12.083182334899902 \n",
      "\n",
      "Step: 28000 Loss: 2.6408653259277344 ACC: 0.296875 Perplexity: 14.025334358215332 \n",
      "\n",
      "Step: 28100 Loss: 1.9894157648086548 ACC: 0.375 Perplexity: 7.31126070022583 \n",
      "\n",
      "Step: 28200 Loss: 1.9968196153640747 ACC: 0.4375 Perplexity: 7.365592956542969 \n",
      "\n",
      "Step: 28300 Loss: 2.2567451000213623 ACC: 0.40625 Perplexity: 9.551947593688965 \n",
      "\n",
      "Step: 28400 Loss: 2.159428119659424 ACC: 0.328125 Perplexity: 8.666180610656738 \n",
      "\n",
      "Step: 28500 Loss: 2.0037310123443604 ACC: 0.390625 Perplexity: 7.416676044464111 \n",
      "\n",
      "Step: 28600 Loss: 2.269141674041748 ACC: 0.328125 Perplexity: 9.671095848083496 \n",
      "\n",
      "Step: 28700 Loss: 2.3529052734375 ACC: 0.296875 Perplexity: 10.516077041625977 \n",
      "\n",
      "Step: 28800 Loss: 1.8994247913360596 ACC: 0.4375 Perplexity: 6.682049751281738 \n",
      "\n",
      "Step: 28900 Loss: 2.064913272857666 ACC: 0.421875 Perplexity: 7.88461446762085 \n",
      "\n",
      "Step: 29000 Loss: 1.889965534210205 ACC: 0.421875 Perplexity: 6.619140148162842 \n",
      "\n",
      "Step: 29100 Loss: 2.2933037281036377 ACC: 0.328125 Perplexity: 9.907615661621094 \n",
      "\n",
      "Step: 29200 Loss: 2.252234697341919 ACC: 0.421875 Perplexity: 9.50896167755127 \n",
      "\n",
      "Step: 29300 Loss: 1.9952889680862427 ACC: 0.46875 Perplexity: 7.35432767868042 \n",
      "\n",
      "Step: 29400 Loss: 2.0488157272338867 ACC: 0.328125 Perplexity: 7.758707046508789 \n",
      "\n",
      "Step: 29500 Loss: 1.8370132446289062 ACC: 0.375 Perplexity: 6.277760028839111 \n",
      "\n",
      "Step: 29600 Loss: 2.397519111633301 ACC: 0.28125 Perplexity: 10.99586296081543 \n",
      "\n",
      "Step: 29700 Loss: 1.990486979484558 ACC: 0.328125 Perplexity: 7.31909704208374 \n",
      "\n",
      "Step: 29800 Loss: 1.9776471853256226 ACC: 0.390625 Perplexity: 7.225722312927246 \n",
      "\n",
      "Step: 29900 Loss: 2.0005788803100586 ACC: 0.4375 Perplexity: 7.393334865570068 \n",
      "\n",
      "Step: 30000 Loss: 2.207754373550415 ACC: 0.3125 Perplexity: 9.095268249511719 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('WandB.pickle', 'rb') as f:\n",
    "        trainVariables = pickle.load(f)\n",
    "except:        \n",
    "    trainVariables = initializer(units, vocab_size, sequenceLength)\n",
    "    trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVpfeDj7AUMo"
   },
   "outputs": [],
   "source": [
    "with open(filename, 'wb') as f:\n",
    "   pickle.dump(trainVariables,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HP1hzOGHvuD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate(steps):\n",
    "    sentence = \"\"\n",
    "    state = tf.zeros([1, units])\n",
    "    seq = [0]\n",
    "\n",
    "    for step in range(steps):\n",
    "\n",
    "        inp = tf.one_hot(seq[-1: ], vocab_size)\n",
    "        # print(\"inp:  \", inp.shape)\n",
    "        op_InpToHidden = tf.matmul(inp, trainVariables[0])\n",
    "        # print(\"op_InpToHidden :  \", op_InpToHidden.shape) \n",
    "        op_HiddenToHidden = tf.matmul(state, trainVariables[2]) + trainVariables[1] \n",
    "        # print(\"op_HiddenToHidden :  \", op_HiddenToHidden.shape)\n",
    "        op_FromHidden = op_InpToHidden + op_HiddenToHidden\n",
    "        # print(\"op_FromHidden :  \", op_FromHidden.shape)\n",
    "        state = tf.nn.tanh(op_FromHidden)\n",
    "        # print(\"op_FromHidden :  \", op_FromHidden.shape)\n",
    "\n",
    "\n",
    "        # Second node\n",
    "        op_FromOPLayer = tf.matmul(state, trainVariables[3]) + trainVariables[4]\n",
    "        y = tf.nn.softmax(op_FromOPLayer)\n",
    "        \n",
    "        y = y.numpy()[0]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        seq.append(np.random.choice(vocab_size, p=y))\n",
    "\n",
    "        char = [ind_to_ch[ind] for ind in seq]\n",
    "    return sentence.join(char)\n",
    "        \n",
    "sentence = generate(2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MWaoUAf8HwHq",
    "outputId": "06bf0b11-1895-4a95-c583-b2e7fcbf99d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>s rade,\n",
      "Thish hoc, anch, to, loth bo knows Pachas I ardard:\n",
      "Anj lay ofFrink's thes who y I will cand arttinfs thes Menstyou they eaglong, sour wall,, that your profsend hald! und oof pain; Aod of youf lows; The, so hat thee, he loyd, Co with Puts foll, Lordus to of fou shreds sfow goch hegh thit thad gan:\n",
      "Uot\n",
      "I splich hof gilot?,\n",
      "Tf As TmUS:\n",
      "Mo, wo pod ey sting,\n",
      "Dhd lay, and toodst whice, by So your. of to hiscoue wigh Ifot worde, the loves, my what thy dod. 's bik, wher you, bomes's bseid,\n",
      "roue.\n",
      "grat rowendourds: de;-yir upoamps, boow's go walll.\n",
      "\n",
      "CAEN INA:\n",
      "G staln, thotr uastery is for dord, yor uthin ssyid,\n",
      "The batser hod sich, a rold, ond longh has. so prowho hold the eronnspran hes,\n",
      "my merucerof in pose you, Mice of to soll.\n",
      "\n",
      "CLEc:\n",
      "Ptvis,,, ad I thes of woad you, By soune ruth Pucr oors igrd ho apray, lace urr herhour pooth you, harn?\n",
      "Mh wIE: Guth y Goo.\n",
      "\n",
      "BOoT:\n",
      "Sivid sood, ond bet gods and..\n",
      "CyENl:\n",
      "Snclas theer--and not your garded foodst, bete's attislly dhbf,;e your duss?\n",
      "\n",
      "flreder, the batinae.\n",
      "Thirg no drise in pout;\n",
      "Fhr warr in asdropr,e\n",
      "for should now my ceimsy.\n",
      "\n",
      "KyNUS:\n",
      "And manmer and cence?\n",
      "ngrrghffo thour's, and and sheal hau,\n",
      "Andellod, nourn!\n",
      "\n",
      "CKEN:\n",
      "Salv sell mifron:\n",
      "The you sirstry hoves I pinn, hits be prodd, whend shoal frealo Bh what is offer.\n",
      "Thas thin, as harts, thes; thot hreplteims, whin yore this shoher oof I have; If lordse ond doth hore.\n",
      "\n",
      "MOGNesSThGON K:\n",
      "Hor ar, the the-sss,\n",
      "As: Proum bient Protiy'd, and lotfrriseg, asd luct froted\n",
      " now vell'd do a ofttoml ifor's the groulk,\n",
      "Asdom speesh prowes shes it anth Gheh no till yer co is pixcand a-dig of Cood, oes preyshored thea;\n",
      "AThe ird, should if you shilds, hos, hast?\n",
      "Fith Pererua thae fordof uf norake,\n",
      "Yod sianlingly.\n",
      "\n",
      "DRI CHALE:\n",
      "Hed then thy pany,\n",
      "Thow gecd, hore Bpotly hotlls and!'s my -ofee doud had uo-e, of she soull you sfofth Fo;.\n",
      "\n",
      "PENRUS:\n",
      "My, ouse,\n",
      "wf all, hid shene to Foms foegur me and lorst.'\n",
      "fo! you beot a. Thcenc, yo amacl that I\n",
      "late and off the bea-drou har,s, all.gordser, warh y,\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "soUAzBYgm3Xd"
   },
   "source": [
    "#Reference\n",
    "1. https://github.com/vzhou842/rnn-from-scratch\n",
    "2. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n",
    "3. https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IDLEx5_Submission.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
