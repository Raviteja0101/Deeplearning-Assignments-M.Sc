{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TensorBoard.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfh6Mg5lQL7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from datasets import MNISTDataset\n",
        "from time import time\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMPAS_kpRc6g",
        "colab_type": "text"
      },
      "source": [
        "#Setting Up Log File for Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVXg3ZgpRehe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                     batch_size=256,\n",
        "                     seed=int(time())\n",
        "                     )\n",
        "\n",
        "# first change: set up log dir and file writer(s)\n",
        "import time\n",
        "logdir1 = os.path.join(\"logs1\", \"linear\" + str(time.time()))\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir1, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(logdir1, \"test\"))\n",
        "gradient_writer = tf.summary.create_file_writer(os.path.join(logdir1, \"gradient\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoDFdoAOQ7Jp",
        "colab_type": "text"
      },
      "source": [
        "##Model 1\n",
        "\n",
        "Here are a few suggestions and their results\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "with regularizations\n",
        "\n",
        "lr=0.0025\n",
        "\n",
        "batch=128\n",
        "\n",
        "Loss: 0.556975245475769 Accuracy: 0.796875\n",
        "\n",
        "Final test accuracy: 0.8029999732971191\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "lr = 0.0025\n",
        "\n",
        "no regularizations\n",
        "\n",
        "batch sizea=128\n",
        "\n",
        "Train accuracy: 0.875\n",
        "\n",
        "Test accuracy: 0.8687999844551086\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "batch=128\n",
        "\n",
        "Loss: 0.7130742073059082 Accuracy: 0.7578125\n",
        "\n",
        "Final test accuracy: 0.7803999781608582\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qumg9B89Q_IF",
        "colab_type": "code",
        "outputId": "93af2791-12e2-457b-de7b-08642765aa61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "# define the model first, from input to output\n",
        "\n",
        "# this is a super deep model, cool!\n",
        "n_units = 100\n",
        "n_layers = 8\n",
        "w_range = 0.4\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "    \n",
        "print(tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    \n",
        "    \n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "        gradients = tf.norm(grad, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "\n",
        "    # change #2: log this stuff every time step (rather wasteful)\n",
        "    with train_writer.as_default():\n",
        "          tf.summary.scalar(\"loss\", xent, step=step)\n",
        "          tf.summary.histogram(\"logits\", out, step=step)\n",
        "          tf.summary.histogram(\"weights\", var, step=step)\n",
        "\n",
        "    with gradient_writer.as_default():\n",
        "      tf.summary.scalar(\"gradients\", gradients, step=step)\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "\n",
        "        # change #3: log this only once every 100 steps\n",
        "        with test_writer.as_default():\n",
        "            tf.summary.scalar(\"accuracy\", acc, step=step)\n",
        "            tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    out = mnist.test_data\n",
        "    for layer in layers:\n",
        "      out = layer(out)\n",
        "    test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "    test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "    with test_writer.as_default():\n",
        "      tf.summary.scalar(\"accuracy\", test_acc, step=step)\n",
        "print(\"Final test accuracy: {}\".format(test_acc))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.ops.init_ops_v2.RandomUniform object at 0x7f9dbacfbba8>\n",
            "Loss: 129.01025390625 Accuracy: 0.11328125\n",
            "Loss: nan Accuracy: 0.10546875\n",
            "Loss: nan Accuracy: 0.08203125\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.1171875\n",
            "Loss: nan Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.1171875\n",
            "Loss: nan Accuracy: 0.09375\n",
            "Loss: nan Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.09765625\n",
            "Loss: nan Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.09375\n",
            "Loss: nan Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.0859375\n",
            "Loss: nan Accuracy: 0.1015625\n",
            "Loss: nan Accuracy: 0.05859375\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.0859375\n",
            "Loss: nan Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.06640625\n",
            "Loss: nan Accuracy: 0.07421875\n",
            "Starting new epoch...\n",
            "Loss: nan Accuracy: 0.12109375\n",
            "Final test accuracy: 0.09799999743700027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_s826hi1QU",
        "colab_type": "text"
      },
      "source": [
        "#Inference from model 1\n",
        "The gradient plot reveals an exploding gradient problem which is caused by large variance during weight initialization which can also be dependent on the network architecture, this theory is suported by this  [paper](https://arxiv.org/pdf/1712.08969.pdf). A quick fix is to reduce the learning rate, change the activation function to a non linear activation function, (as relu is linear funtion and allows exploding gradients) or alter the network architecture(reduce number of units to 50).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQxNWXKLVPSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then load/run tensorboard\n",
        "\n",
        "# %load_ext tensorboard\n",
        "\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNKj_6-NVQmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3alJ0DtqufQ",
        "colab_type": "text"
      },
      "source": [
        "##Model 2\n",
        "Here are a few suggestions and their results\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "lr=0.1\n",
        "\n",
        "activation function = relu\n",
        "\n",
        "step=2000\n",
        "\n",
        "Loss: 0.2918747663497925 Accuracy: 0.9296875\n",
        "\n",
        "Final test accuracy: 0.9363999962806702\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "lr=0.1\n",
        "\n",
        "activation function = relu\n",
        "\n",
        "step=4000\n",
        "\n",
        "Loss: 0.0701780766248703 Accuracy: 0.984375\n",
        "\n",
        "Final test accuracy: 0.9546999931335449\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhtwbON6qxMx",
        "colab_type": "code",
        "outputId": "d1eb2cd6-67ef-4f31-d65a-d0475adbc2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "from time import time\n",
        "\n",
        "# first change: set up log dir and file writer(s)\n",
        "import time\n",
        "logdir2 = os.path.join(\"logs2\", \"linear\" + str(time.time()))\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir2, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(logdir2, \"test\"))\n",
        "gradient_writer = tf.summary.create_file_writer(os.path.join(logdir2, \"gradient\"))\n",
        "\n",
        "\n",
        "\n",
        "# define the model first, from input to output\n",
        "\n",
        "# this is a super deep model, cool!\n",
        "n_units = 100\n",
        "n_layers = 8\n",
        "w_range = 0.1\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.sigmoid,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "   \n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "        gradients = tf.norm(grad, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "\n",
        "    \n",
        "    # change #2: log this stuff every time step (rather wasteful)\n",
        "    with train_writer.as_default():\n",
        "      tf.summary.scalar(\"loss\", xent, step=step)\n",
        "      tf.summary.histogram(\"logits\", out, step=step)\n",
        "      tf.summary.histogram(\"weights\", var, step=step)\n",
        "\n",
        "\n",
        "\n",
        "    with gradient_writer.as_default():\n",
        "        tf.summary.scalar(\"gradients\", gradients, step=step)\n",
        "\n",
        "\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "    # change #3: log this only once every 100 steps\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"accuracy\", acc, step=step)\n",
        "        tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n",
        "\n",
        "\n",
        "    out = mnist.test_data\n",
        "    for layer in layers:\n",
        "      out = layer(out)\n",
        "    test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "    test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "    with test_writer.as_default():\n",
        "      tf.summary.scalar(\"accuracy\", test_acc, step=step)\n",
        "print(\"Final test accuracy: {}\".format(test_acc))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3667476177215576 Accuracy: 0.08984375\n",
            "Loss: 2.2939462661743164 Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: 2.3090362548828125 Accuracy: 0.09765625\n",
            "Loss: 2.295132637023926 Accuracy: 0.12109375\n",
            "Starting new epoch...\n",
            "Loss: 2.3097448348999023 Accuracy: 0.10546875\n",
            "Loss: 2.307443141937256 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.302980899810791 Accuracy: 0.09375\n",
            "Loss: 2.3050084114074707 Accuracy: 0.06640625\n",
            "Loss: 2.2986652851104736 Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: 2.3082869052886963 Accuracy: 0.1015625\n",
            "Loss: 2.308178424835205 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.290656566619873 Accuracy: 0.140625\n",
            "Loss: 2.307791233062744 Accuracy: 0.12109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2986695766448975 Accuracy: 0.12109375\n",
            "Loss: 2.301567792892456 Accuracy: 0.09765625\n",
            "Loss: 2.2965731620788574 Accuracy: 0.13671875\n",
            "Starting new epoch...\n",
            "Loss: 2.304795026779175 Accuracy: 0.09765625\n",
            "Loss: 2.306703567504883 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.306769609451294 Accuracy: 0.109375\n",
            "Loss: 2.2965757846832275 Accuracy: 0.13671875\n",
            "Starting new epoch...\n",
            "Final test accuracy: 0.09799999743700027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLJlw6GGyuQd",
        "colab_type": "text"
      },
      "source": [
        "#Inference from model 2\n",
        "Here, the gradient plot reveals a vanishing gradient problem, caused by the sigmoid function streamling all input to [0, 1], that can be resolved by using a linear activation function like relu [max, 0]. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNf5wQ4_tSBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then load/run tensorboard\n",
        "\n",
        "# %load_ext tensorboard\n",
        "\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OZEKYsjw1Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us61c0Pz5JBm",
        "colab_type": "text"
      },
      "source": [
        "##Model 3\n",
        "\n",
        "kernerInitializer - maxval = 0.01\n",
        "\n",
        "Loss: 0.3130064606666565 Accuracy: 0.93359375\n",
        "\n",
        "Final test accuracy: 0.9319000244140625\n",
        "\n",
        "---\n",
        "\n",
        "kernerInitializer - maxval = 0.01\n",
        "\n",
        "steps = 4000\n",
        "\n",
        "Loss: 0.10393001139163971 Accuracy: 0.97265625\n",
        "\n",
        "Final test accuracy: 0.9617000222206116\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IizmDPDd5Mxl",
        "colab_type": "code",
        "outputId": "0a09e4a6-3bfe-430d-8601-985f0425516f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "# first change: set up log dir and file writer(s)\n",
        "import time\n",
        "import os\n",
        "logdir3 = os.path.join(\"logs3\", \"linear3\" + str(time.time()))\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir3, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(logdir3, \"test\"))\n",
        "gradient_writer = tf.summary.create_file_writer(os.path.join(logdir3, \"gradient\"))\n",
        "\n",
        "\n",
        "# let's use fewer layers...\n",
        "n_units = 100\n",
        "n_layers = 2\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01)))\n",
        "\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "    gradients = tf.norm(grad, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "\n",
        "    \n",
        "    with gradient_writer.as_default():\n",
        "      tf.summary.scalar(\"gradients\", gradients, step=step)\n",
        "  \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "    # change #3: log this only once every 100 steps\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"accuracy\", acc, step=step)\n",
        "        tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n",
        "\n",
        "\n",
        "    # change #2: log this stuff every time step (rather wasteful)\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"loss\", xent, step=step)\n",
        "        tf.summary.histogram(\"logits\", out, step=step)\n",
        "        tf.summary.histogram(\"weights\", var, step=step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    out = mnist.test_data\n",
        "    for layer in layers:\n",
        "      out = layer(out)\n",
        "    test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "    test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "    with test_writer.as_default():\n",
        "      tf.summary.scalar(\"accuracy\", test_acc, step=step)\n",
        "print(\"Final test accuracy: {}\".format(test_acc))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3025851249694824 Accuracy: 0.109375\n",
            "Loss: 2.299927234649658 Accuracy: 0.1171875\n",
            "Loss: 2.3056793212890625 Accuracy: 0.06640625\n",
            "Starting new epoch...\n",
            "Loss: 2.303879737854004 Accuracy: 0.1328125\n",
            "Loss: 2.3015518188476562 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.301048994064331 Accuracy: 0.1015625\n",
            "Loss: 2.299473524093628 Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: 2.2982640266418457 Accuracy: 0.1171875\n",
            "Loss: 2.306756019592285 Accuracy: 0.09765625\n",
            "Loss: 2.3043737411499023 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.2929465770721436 Accuracy: 0.15625\n",
            "Loss: 2.305248498916626 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.3037900924682617 Accuracy: 0.09765625\n",
            "Loss: 2.30401349067688 Accuracy: 0.09375\n",
            "Loss: 2.303297758102417 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.297632932662964 Accuracy: 0.12109375\n",
            "Loss: 2.3020265102386475 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.3050284385681152 Accuracy: 0.09765625\n",
            "Loss: 2.3008852005004883 Accuracy: 0.12109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2982616424560547 Accuracy: 0.1171875\n",
            "Final test accuracy: 0.11349999904632568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDIRcOrN7MKp",
        "colab_type": "text"
      },
      "source": [
        "#Inference from model 3\n",
        "We experience a vanishing gradient promblem here, caused by a non zero mean in weights initialization and non uniform variance in all the respective layers. change `maxval` to 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMynnGSy7QqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKy6HeDTZTSv",
        "colab_type": "text"
      },
      "source": [
        "##Model 4\n",
        "\n",
        "A standard deviation of 4 is too high and adds too much noise into the data to an extent that the model cannot identify the features of the image\n",
        "The average standard deviation observed during research is between 0 and 1\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Std Dev = 1\n",
        "\n",
        "Loss: 0.7794443368911743 Accuracy: 0.75\n",
        "\n",
        "Final test accuracy: 0.9099000096321106\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Std Dev = 0.5\n",
        "\n",
        "Loss: 0.4842979907989502 Accuracy: 0.84375\n",
        "\n",
        "Final test accuracy: 0.9297999739646912\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Std Dev = 0.25\n",
        "\n",
        "Loss: 0.33627036213874817 Accuracy: 0.87890625\n",
        "\n",
        "Final test accuracy: 0.9330000281333923\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZOqM1snZWN2",
        "colab_type": "code",
        "outputId": "4fb9d045-a809-4ab3-a7d6-7daa099382fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# first change: set up log dir and file writer(s)\n",
        "import time\n",
        "logdir4 = os.path.join(\"logs4\", \"linear4\" + str(time.time()))\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir4, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(logdir4, \"test\"))\n",
        "gradient_writer = tf.summary.create_file_writer(os.path.join(logdir4, \"gradient\"))\n",
        "\n",
        "\n",
        "# let's use fewer layers...\n",
        "n_units = 100\n",
        "n_layers = 2\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01)))\n",
        "\n",
        "\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    # I hear adding random noise to inputs helps with generalization!\n",
        "    img_batch = img_batch + tf.random.normal(tf.shape(img_batch), stddev=0.4)\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "    gradients = tf.norm(grad, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "\n",
        "    \n",
        "    with gradient_writer.as_default():\n",
        "      tf.summary.scalar(\"gradients\", gradients, step=step)\n",
        "  \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "    # change #3: log this only once every 100 steps\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"accuracy\", acc, step=step)\n",
        "        tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n",
        "\n",
        "\n",
        "    # change #2: log this stuff every time step (rather wasteful)\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"loss\", xent, step=step)\n",
        "        tf.summary.histogram(\"logits\", out, step=step)\n",
        "        tf.summary.histogram(\"weights\", var, step=step)\n",
        "\n",
        "    out = mnist.test_data\n",
        "    for layer in layers:\n",
        "      out = layer(out)\n",
        "    test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "    test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "    with test_writer.as_default():\n",
        "      tf.summary.scalar(\"accuracy\", test_acc, step=step)\n",
        "print(\"Final test accuracy: {}\".format(test_acc))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.302567958831787 Accuracy: 0.09765625\n",
            "Loss: 2.3015096187591553 Accuracy: 0.0859375\n",
            "Starting new epoch...\n",
            "Loss: 2.2999794483184814 Accuracy: 0.125\n",
            "Loss: 2.301260471343994 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.289503574371338 Accuracy: 0.14453125\n",
            "Loss: 2.299851894378662 Accuracy: 0.0859375\n",
            "Starting new epoch...\n",
            "Loss: 2.093125343322754 Accuracy: 0.25\n",
            "Loss: 1.2149486541748047 Accuracy: 0.546875\n",
            "Loss: 0.8785444498062134 Accuracy: 0.6875\n",
            "Starting new epoch...\n",
            "Loss: 0.7454255223274231 Accuracy: 0.7578125\n",
            "Loss: 0.6598219275474548 Accuracy: 0.77734375\n",
            "Starting new epoch...\n",
            "Loss: 0.6503037214279175 Accuracy: 0.80859375\n",
            "Loss: 0.6118203401565552 Accuracy: 0.76953125\n",
            "Starting new epoch...\n",
            "Loss: 0.6084169745445251 Accuracy: 0.828125\n",
            "Loss: 0.5856684446334839 Accuracy: 0.80859375\n",
            "Loss: 0.5546771287918091 Accuracy: 0.8515625\n",
            "Starting new epoch...\n",
            "Loss: 0.46970653533935547 Accuracy: 0.8203125\n",
            "Loss: 0.37128424644470215 Accuracy: 0.89453125\n",
            "Starting new epoch...\n",
            "Loss: 0.4478006362915039 Accuracy: 0.859375\n",
            "Loss: 0.37780502438545227 Accuracy: 0.8828125\n",
            "Starting new epoch...\n",
            "Final test accuracy: 0.9283000230789185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb9dlXgNaSsp",
        "colab_type": "text"
      },
      "source": [
        "#Inference on Model 4\n",
        "\n",
        "The weights are properly centered as opposed to model 3. The addition of noise makes it difficult for the network to memorize the data, such that with a final training accuracy of 0.28, we have the test accuracy of 0.74. To increase test accuracy feed the network a noise of 0.1 standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmxSQ9W0aRpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcQLouckcSn6",
        "colab_type": "text"
      },
      "source": [
        "##Model 5\n",
        "\n",
        "Remove the unnecessary activation \n",
        "funtion tf.nn.softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg-ZPscZciWw",
        "colab_type": "code",
        "outputId": "5b0ca0b2-c7f1-4e34-b29f-b1b17ee5a4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "source": [
        "from time import time\n",
        "\n",
        "\n",
        "# first change: set up log dir and file writer(s)\n",
        "import time\n",
        "logdir5 = os.path.join(\"logs5\", \"linear5\" + str(time.time()))\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir5, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(logdir5, \"test\"))\n",
        "gradient_writer = tf.summary.create_file_writer(os.path.join(logdir5, \"gradient\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# let's use fewer layers...\n",
        "n_units = 100\n",
        "n_layers = 2\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the softmax output layer :))\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, activation=tf.nn.elu,\n",
        "    kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                     maxval=0.01)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "    gradients = tf.norm(grad, ord='euclidean', axis=None, keepdims=None, name=None)\n",
        "\n",
        "    \n",
        "    with gradient_writer.as_default():\n",
        "      tf.summary.scalar(\"gradients\", gradients, step=step)\n",
        "      \n",
        "  \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "    # change #3: log this only once every 100 steps\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"accuracy\", acc, step=step)\n",
        "        tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n",
        "\n",
        "\n",
        "    # change #2: log this stuff every time step (rather wasteful)\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"loss\", xent, step=step)\n",
        "        tf.summary.histogram(\"logits\", out, step=step)\n",
        "        tf.summary.histogram(\"weights\", var, step=step)\n",
        "\n",
        "    out = mnist.test_data\n",
        "    for layer in layers:\n",
        "      out = layer(out)\n",
        "    test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "    test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "    with test_writer.as_default():\n",
        "      tf.summary.scalar(\"accuracy\", test_acc, step=step)\n",
        "print(\"Final test accuracy: {}\".format(test_acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.30259370803833 Accuracy: 0.09375\n",
            "Loss: 2.2945985794067383 Accuracy: 0.1640625\n",
            "Loss: 2.3022637367248535 Accuracy: 0.09765625\n",
            "Starting new epoch...\n",
            "Loss: 2.297896385192871 Accuracy: 0.11328125\n",
            "Loss: 2.2961297035217285 Accuracy: 0.1171875\n",
            "Starting new epoch...\n",
            "Loss: 2.2649710178375244 Accuracy: 0.1953125\n",
            "Loss: 1.3318147659301758 Accuracy: 0.5\n",
            "Starting new epoch...\n",
            "Loss: 0.8630139231681824 Accuracy: 0.67578125\n",
            "Loss: 0.7104894518852234 Accuracy: 0.79296875\n",
            "Loss: 0.6176666617393494 Accuracy: 0.81640625\n",
            "Starting new epoch...\n",
            "Loss: 0.47712016105651855 Accuracy: 0.83984375\n",
            "Loss: 0.48497384786605835 Accuracy: 0.87109375\n",
            "Starting new epoch...\n",
            "Loss: 0.5943899750709534 Accuracy: 0.83984375\n",
            "Loss: 0.4000398516654968 Accuracy: 0.890625\n",
            "Loss: 0.3931143581867218 Accuracy: 0.890625\n",
            "Starting new epoch...\n",
            "Loss: 0.3424515426158905 Accuracy: 0.90625\n",
            "Loss: 0.38362276554107666 Accuracy: 0.8984375\n",
            "Starting new epoch...\n",
            "Loss: 0.3049623370170593 Accuracy: 0.9140625\n",
            "Loss: 0.3414199948310852 Accuracy: 0.91015625\n",
            "Starting new epoch...\n",
            "Loss: 0.24583689868450165 Accuracy: 0.9296875\n",
            "Final test accuracy: 0.9266999959945679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nrxL0ut6wkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGpvpu3acYjQ",
        "colab_type": "text"
      },
      "source": [
        "#Inference on Model 5\n",
        "\n",
        "The use of non linear function `softmax` at the out layer is not recommended. use any of the linear functions for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ZgW444cXVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkMnWIeaQOqU",
        "colab_type": "text"
      },
      "source": [
        "#Data Handling with `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N463q-pQfSs",
        "colab_type": "code",
        "outputId": "107c1a4b-82a8-4379-eb32-409eeb9dc886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import time \n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "\n",
        "#Normalization and Flattening\n",
        "train_images = (train_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "test_images = (test_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "\n",
        "\n",
        "#Convsersion of Labels into integers\n",
        "train_labels = train_labels.astype(np.int32)\n",
        "test_labels = test_labels.astype(np.int32)\n",
        "\n",
        "\n",
        "\n",
        "# this is now different\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "\n",
        "train_data = train_data.repeat()\n",
        "train_data = train_data.shuffle(buffer_size=60000)\n",
        "train_data = train_data.batch(128)\n",
        "\n",
        "t1= time.time()\n",
        "\n",
        "print(t1-t0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.37395262718200684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_qKySqYSI_",
        "colab_type": "text"
      },
      "source": [
        "#Shuffle, Repeat and Batch Size\n",
        "\n",
        "To determine `buffer_size`, use the entire length of the dataset, the bigger the better. One disadvantage is that your dataset now takes longer to load. To avoid this disadvantage, do the shuffling and repeating during splitting (contains only file names here) and not when loading.\n",
        "\n",
        "\n",
        "With the below configuration:\n",
        "\n",
        "```python\n",
        "data = tf.data.Dataset.range(4)\n",
        "data = data.repeat(2) \n",
        "data = data.batch(2)  \n",
        "data = data.shuffle(4)\n",
        "```\n",
        "`Shuffle, Repeat, Batch Size` = [2, 1, 3, 0, 3, 0, 2, 1]\n",
        "\n",
        "`Shuffle,  Batch Size, Repeat` = [2, 3, 0, 1, 2, 1, 3, 0]\n",
        "\n",
        "`Batch Size, Shuffle, Repeat` = [0, 1, 2, 3, 0, 1, 2, 3]\n",
        "\n",
        "`Batch Size, Repeat, Shuffle` = [2, 3, 0, 1, 2, 3, 0, 1]\n",
        "\n",
        "`Repeat, Shuffle, Batch Size` = [1, 3, 1, 0, 2, 3, 2, 0]\n",
        "\n",
        "`Repeat, Batch Size, Shuffle` = [0, 1, 2, 3, 0, 1, 2, 3]\n",
        "\n",
        "1. Shuffle, Repeat, Batch Size: Makes you see all the training example in one epoch in no particular order\n",
        "2. Shuffle,  Batch Size, Repeat = Makes you see all the training example in one epoch  in no particular order\n",
        "3. Batch Size, Shuffle, Repeat = Makes you see all the training example in one epoch and the next in an orderly form\n",
        "4. Batch Size, Repeat, Shuffle = Makes you see all the training example in one epoch and the next in an orderly form\n",
        "5. Repeat, Shuffle, Batch Size = Reduces the chances of seeing all examples of the dataset in one epoch.\n",
        "6. Repeat, Batch Size, Shuffle = Makes you see all the training example in one epoch and the next in an orderly form\n",
        "\n",
        "NB: Looking at any pipeline, one can easily determine if a network is feeding based or queue-based."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufTURgr6uHqJ",
        "colab_type": "text"
      },
      "source": [
        "# Experimenting with TF Data on the model\n",
        "In this function we perform series of **Repeat,shuffle, batch** operations using tf.data on the dataset\n",
        "\n",
        "We also make use **Iterator** to iterate over the batch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftoLkzz9uFDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processData():\n",
        "  import tensorflow as tf\n",
        "  import numpy as np\n",
        "  import os\n",
        "  mnist = tf.keras.datasets.mnist\n",
        "  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "  #Normalization and Flattening\n",
        "  train_images = (train_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "  test_images = (test_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "\n",
        "  #Convsersion of Labels into integers\n",
        "  train_labels = train_labels.astype(np.int32)\n",
        "  test_labels = test_labels.astype(np.int32)\n",
        "\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "  \n",
        "  # Entire train data is repeated twice, i.e if the size of dataset is 10,000\n",
        "  # repeat makes 2 copies each element and shuffles it.\n",
        "  # Shuffle takes the batch of first 1000 elements and batch selects 64 elements\n",
        "  train_data = train_data.repeat(2).shuffle(buffer_size=1000).batch(64)\n",
        "  experimentWithTFData(train_data, test_images,test_labels )\n",
        "\n",
        "  # Get first 1000  images and shuffle them, after shuffling a batch of 64 images is extracted and \n",
        "  # all the images of batch are repeated four times. The ouput will consist of 64*4 images\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "  train_data = train_data.shuffle(buffer_size=1000).batch(64).repeat(4)\n",
        "  experimentWithTFData(train_data, test_images,test_labels )\n",
        "\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "  train_data = train_data.repeat(2).batch(64).shuffle(buffer_size=1000)\n",
        "  experimentWithTFData(train_data, test_images,test_labels )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ51mm2X3uK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experimentWithTFData(train_data, test_images,test_labels):\n",
        "  import os\n",
        "  import time\n",
        "  import tensorflow\n",
        "  tfDataLogDir = os.path.join(\"tfData\", \"linear3\" + str(time.time()))\n",
        "  train_writer = tf.summary.create_file_writer(os.path.join(tfDataLogDir, \"train\"))\n",
        "  test_writer = tf.summary.create_file_writer(os.path.join(tfDataLogDir, \"test\"))\n",
        "\n",
        "  # let's use fewer layers...\n",
        "  n_units = 100\n",
        "  n_layers = 2\n",
        "\n",
        "  # just set up a \"chain\" of hidden layers\n",
        "  layers = []\n",
        "  for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                        maxval=0.),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "    # finally add the output layer\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                            maxval=0.01)))\n",
        "  \n",
        "  lr = 0.001\n",
        "  for step in range(2000):\n",
        "    img_batch, lbl_batch = next(iter(train_data))\n",
        "    with tf.GradientTape() as tape:\n",
        "    # here we just run all the layers in sequence via a for-loop\n",
        "      out = img_batch\n",
        "      for layer in layers:\n",
        "        out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "      var.assign_sub(lr*grad)      \n",
        "    i=0        \n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar(\"loss\", xent, step=step)\n",
        "        tf.summary.histogram(\"logits\", out, step=step)\n",
        "        for W in weights:\n",
        "            tf.summary.histogram(\"weights\"+str(i), W, step=step)\n",
        "            i+=1\n",
        "    tf.summary.scalar(\"Loss\", xent, step=step)    \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "        with train_writer.as_default():\n",
        "            tf.summary.scalar(\"Trainaccuracy\", acc, step=step)\n",
        "            tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n",
        "\n",
        "  out = test_images\n",
        "  for layer in layers:\n",
        "    out = layer(out)\n",
        "    test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "    test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar(\"accuracy\", test_acc, step=step)\n",
        "  print(\"Final test accuracy: {}\".format(test_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaWkTbCuyS6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx31OCxnVodG",
        "colab_type": "text"
      },
      "source": [
        "# Visualization\n",
        "We try to visualize one of the weight vectors and how it gets updated over time and create an animation gif"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YUMX6qGV7tf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "9f91c765-b20a-42e0-cef1-ff389be89f04"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "from datasets import MNISTDataset\n",
        "import time\n",
        "\n",
        "os.getcwd()\n",
        "logdir = os.path.join(\"logs\", \"linear\" + str(time.time()))\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))\n",
        "\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                     batch_size=128, seed=int(time.time()))\n",
        "n_units = 100\n",
        "n_layers = 8\n",
        "w_range = 0.4\n",
        "\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "  layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range),\n",
        "        bias_initializer=tf.initializers.constant(0.001),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(0.1)))\n",
        "\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range),\n",
        "        bias_initializer=tf.initializers.constant(0.001),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(0.1)))\n",
        "\n",
        "weightHistory = []\n",
        "lr = 0.0025\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "    i=0        \n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "    History = [layers[0].trainable_variables[0]]\n",
        "    weightHistory.append(History)\n",
        "out = mnist.test_data\n",
        "for layer in layers:\n",
        "    out = layer(out)\n",
        "test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "print(\"Final test accuracy: {}\".format(acc))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 141.35494995117188 Accuracy: 0.1015625\n",
            "Loss: 2.8934600353240967 Accuracy: 0.3203125\n",
            "Loss: 1.7831906080245972 Accuracy: 0.5390625\n",
            "Loss: 1.616523265838623 Accuracy: 0.453125\n",
            "Loss: 1.0778954029083252 Accuracy: 0.6484375\n",
            "Starting new epoch...\n",
            "Loss: 0.849216103553772 Accuracy: 0.7265625\n",
            "Loss: 1.0690202713012695 Accuracy: 0.6796875\n",
            "Loss: 1.0295464992523193 Accuracy: 0.7109375\n",
            "Loss: 1.154308795928955 Accuracy: 0.6015625\n",
            "Loss: 0.8497653007507324 Accuracy: 0.734375\n",
            "Starting new epoch...\n",
            "Loss: 0.8377212285995483 Accuracy: 0.7421875\n",
            "Loss: 0.9917375445365906 Accuracy: 0.6875\n",
            "Loss: 0.7171486616134644 Accuracy: 0.7578125\n",
            "Loss: 0.737640380859375 Accuracy: 0.734375\n",
            "Loss: 0.7550614476203918 Accuracy: 0.7265625\n",
            "Starting new epoch...\n",
            "Loss: 0.43732765316963196 Accuracy: 0.8125\n",
            "Loss: 0.8233800530433655 Accuracy: 0.71875\n",
            "Loss: 0.7033606171607971 Accuracy: 0.765625\n",
            "Loss: 0.6245964169502258 Accuracy: 0.796875\n",
            "Starting new epoch...\n",
            "Loss: 0.7371636629104614 Accuracy: 0.7578125\n",
            "Final test accuracy: 0.7990000247955322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67uXAITfYIix",
        "colab_type": "text"
      },
      "source": [
        "Create a matrix of zeros and initialize the board with the weight. And then call the animate method to iterate over the weight updations and create an animation of the weight vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwPC8EkRYG4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "5d0fba85-3769-45f6-b956-b0cb1623b2b8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "len(weightHistory)\n",
        "\n",
        "x = 784               # board will be X by Y where X = boardsize\n",
        "y = 100\n",
        "pad = 0               # padded border, do not change this!\n",
        "initial_cells = 1500  # this number of initial cells will be placed in randomly generated positions\n",
        "my_board = np.zeros((x+pad,y+pad))\n",
        "my_board = weightHistory[25][0].numpy()\n",
        "fig = plt.gcf()\n",
        "im = plt.imshow(my_board)\n",
        "plt.show()\n",
        "\n",
        "i = 0\n",
        "def animate(frame):\n",
        "    if not frame % 100:\n",
        "      print(\"Adding frame\",frame)\n",
        "    my_board = np.array(weightHistory[frame][0].numpy())\n",
        "    im.set_data(my_board)\n",
        "    return im\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, frames=1000,interval=10)\n",
        "anim.save('animation.mp4')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Adding frame 0\n",
            "Adding frame 0\n",
            "Adding frame 100\n",
            "Adding frame 200\n",
            "Adding frame 300\n",
            "Adding frame 400\n",
            "Adding frame 500\n",
            "Adding frame 600\n",
            "Adding frame 700\n",
            "Adding frame 800\n",
            "Adding frame 900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtAWBQTSb2Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}